# API Flask pour extraction des prix de m√©taux depuis Shmet
"""
Installation requise:
pip install flask flasgger scrapy scrapy-playwright twisted[tls] pyopenssl service_identity parsel psycopg2-binary apscheduler
playwright install chromium

VERSION STABLE CORRIG√âE POUR D√âPLOIEMENT
"""
import sys
import os
import time
from datetime import datetime
import logging
from threading import Thread, Lock, Event
from queue import Queue, Empty
import re
import json
import traceback

# ==============================
# INSTALLATION DU REACTOR ASYNCIO
# ==============================
# L'installation doit se faire avant tout import de Twisted si possible
from twisted.internet import asyncioreactor
try:
    asyncioreactor.install()
except Exception as e:
    # Le reactor pourrait √™tre d√©j√† install√© si le processus est r√©utilis√© (moins fr√©quent en prod)
    pass
    
from twisted.internet import reactor, defer

from flask import Flask, jsonify, request
from flasgger import Swagger
import psycopg2
from psycopg2.extras import RealDictCursor
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger

# Import Scrapy
import scrapy
from parsel import Selector
from scrapy.crawler import CrawlerRunner
from scrapy.utils.log import configure_logging
from scrapy_playwright.page import PageMethod

# ==============================
# CONFIGURATION DU LOGGING
# ==============================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# R√©duire logs verbeux
logging.getLogger('scrapy').setLevel(logging.ERROR)
logging.getLogger('filelock').setLevel(logging.ERROR)
logging.getLogger('playwright').setLevel(logging.ERROR)
logging.getLogger('apscheduler').setLevel(logging.INFO) # On garde les logs du scheduler pour le suivi

logger.info("="*80)
logger.info("üöÄ D√âMARRAGE APPLICATION - VERSION STABLE CORRIG√âE")
logger.info("="*80)

# ==============================
# CONFIGURATION BASE DE DONN√âES
# ==============================
# REMARQUE : Utiliser des variables d'environnement est fortement recommand√© en production
DB_CONFIG = {
    "user": "administrationSTS",
    "password": "St$@0987",
    "host": "avo-adb-002.postgres.database.azure.com",
    "port": "5432",
    "database": "LME_DB",
    "sslmode": "require"
}

METAL_MAPPING = {
    "Cu cathode 1#": "copper",
    "Zn ingot 0#, Shanghai": "zinc",
    "Tin ingot 1#(99.9%),East China": "tin"
}

URL_BASE = "https://en.shmet.com/Home"

TARGETS = [
    "Cu cathode 1#",
    "Zn ingot 0#, Shanghai",
    "Tin ingot 1#(99.9%),East China",
]

# Variables globales
reactor_ready = Event()  # Event pour signaler que le reactor est pr√™t
scraping_lock = Lock()
runner = None
scheduler = None # D√©clarer globalement ici

logger.info(f"üéØ {len(TARGETS)} produits configur√©s")


# ==============================
# FONCTIONS BASE DE DONN√âES
# ==============================
def get_db_connection():
    """Cr√©er une connexion √† la base de donn√©es."""
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        return conn
    except Exception as e:
        logger.error(f"‚ùå Erreur connexion DB: {e}")
        raise

def save_prices_to_db(data, source_url=URL_BASE, price_datetime=None):
    """Enregistrer les prix dans la base de donn√©es."""
    conn = None
    inserted_count = 0
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if price_datetime is None:
            price_datetime = datetime.now()
        
        price_date = price_datetime.date()
        
        for product_name, price in data.items():
            if price is None:
                continue
            
            metal_type = METAL_MAPPING.get(product_name)
            if not metal_type:
                continue
            
            insert_query = """
                INSERT INTO metal_prices 
                (source_product_name, metal_type, price, currency, unit, source_url, price_date, created_at)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            cursor.execute(insert_query, (
                product_name, metal_type, price, 'CNY', 'ton', source_url, price_date, price_datetime
            ))
            inserted_count += 1
            # logger.info(f"    ‚úÖ {product_name} = {price} CNY (date: {price_datetime})")
            
        conn.commit()
        logger.info(f"‚úÖ {inserted_count} prix enregistr√©s avec date {price_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        return inserted_count
        
    except Exception as e:
        logger.error(f"‚ùå Erreur enregistrement: {e}")
        if conn:
            conn.rollback()
        raise
    finally:
        if conn:
            # Assurez-vous que le curseur et la connexion sont toujours ferm√©s
            try:
                cursor.close()
                conn.close()
            except:
                pass


def log_sync_operation(sync_type, status, metals_updated, error_message=None, duration=None):
    """Enregistrer une op√©ration de synchronisation."""
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        insert_query = """
            INSERT INTO sync_logs 
            (sync_type, status, metals_updated, error_message, duration_seconds, created_at)
            VALUES (%s, %s, %s, %s, %s, NOW())
        """
        
        cursor.execute(insert_query, (sync_type, status, metals_updated, error_message, duration))
        conn.commit()
        logger.info(f"üìù Log: {status} - {metals_updated} m√©taux")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur log: {e}")
        if conn:
            conn.rollback()
    finally:
        if conn:
            try:
                cursor.close()
                conn.close()
            except:
                pass


# ==============================
# EXTRACTION
# ==============================
def extract_from_dom(response: scrapy.http.Response):
    """Extraction depuis le DOM rendu."""
    sel = Selector(text=response.text)
    data = {target: None for target in TARGETS}
    
    sections = sel.xpath('//div[contains(@class, "card-title") and contains(@class, "pull-left")]')
    
    china_section = None
    for section in sections:
        title = section.xpath('string(.)').get("").strip()
        if "China Domestic Market Price" in title:
            china_section = section
            break
    
    if not china_section:
        logger.warning("‚ö†Ô∏è  Section 'China Domestic Market Price' non trouv√©e")
        return data
    
    # Naviguer jusqu'au parent 'card' ou 'panel'
    parent_card = china_section.xpath('ancestor::div[contains(@class, "card") or contains(@class, "panel")]')
    if not parent_card:
        logger.warning("‚ö†Ô∏è  Parent card non trouv√© pour la section")
        return data
        
    rows = parent_card[0].css("tr.el-table__row")
    # logger.info(f"    üìä {len(rows)} lignes trouv√©es")
    
    for row in rows:
        name_el = row.css("td span.cell-name")
        # Tentative d'extraction de la 3√®me colonne pour la valeur (la colonne "price" dans le tableau)
        val_el = row.css("td.el-table_1_column_3 div.cell")
        
        if not name_el or not val_el:
            # Fallback g√©n√©rique si la structure change l√©g√®rement
            val_el = row.css("td:nth-child(3) div.cell")
        
        if not name_el or not val_el:
            continue
        
        name = name_el.xpath("string(.)").get("").strip()
        raw_value = val_el.xpath("string(.)").get("").strip()
        
        clean_value = raw_value.replace(",", "")
        # Regex pour ne garder que le nombre (y compris le signe moins et le point)
        numeric_value = re.sub(r"[^\d\.\-]", "", clean_value)
        
        if not numeric_value:
            continue
        
        try:
            price = float(numeric_value)
            
            for target in TARGETS:
                if target.lower().strip() == name.lower().strip(): # Match exact ou proche
                    data[target] = price
                    # logger.info(f"    ‚úÖ {target} = {price}")
                    break
        except ValueError:
            continue
    
    found = sum(1 for v in data.values() if v is not None)
    logger.info(f"‚úÖ {found}/{len(TARGETS)} extraits du DOM")
    return data


# ==============================
# SPIDER SCRAPY
# ==============================
class ShmetSpider(scrapy.Spider):
    name = "shmet_spider"
    
    # Settings pour l'environnement de production
    custom_settings = {
        "DOWNLOAD_HANDLERS": {
            "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
            "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
        },
        "PLAYWRIGHT_BROWSER_TYPE": "chromium",
        "PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT": 60000,
        "PLAYWRIGHT_LAUNCH_OPTIONS": {
            # Ces arguments sont CRUCIAUX pour l'ex√©cution sans GUI sur Azure
            "headless": True,
            "args": ["--no-sandbox", "--disable-setuid-sandbox", "--disable-dev-shm-usage", "--single-process"] 
        },
        "USER_AGENT": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "ROBOTSTXT_OBEY": False,
        "LOG_ENABLED": False,
        "CONCURRENT_REQUESTS": 1,
        "DOWNLOAD_DELAY": 1, # R√©duit le d√©lai pour acc√©l√©rer l'ex√©cution
        "DOWNLOAD_TIMEOUT": 60,
    }
    
    def __init__(self, result_callback=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.result_callback = result_callback
    
    def start_requests(self):
        logger.info(f"üåê Requ√™te Playwright planifi√©e pour: {URL_BASE}")
        yield scrapy.Request(
            url=URL_BASE,
            meta={
                "playwright": True,
                "playwright_page_methods": [
                    # Attendre que les lignes de tableau soient rendues
                    PageMethod("wait_for_selector", "tr.el-table__row", timeout=30000), 
                    # D√©lai suppl√©mentaire pour le chargement des donn√©es (souvent n√©cessaire pour JS/Vue/React)
                    PageMethod("wait_for_timeout", 3000), 
                ],
                "playwright_include_page": False,
            },
            callback=self.parse,
            errback=self.errback,
            dont_filter=True,
        )
    
    def parse(self, response):
        """Parse la r√©ponse."""
        # logger.info(f"üìÑ Parsing {len(response.text)} chars")
        
        data = extract_from_dom(response)
        
        result = {
            "data": data,
            "url": response.url,
            "timestamp": datetime.now().isoformat(),
        }
        
        if self.result_callback:
            self.result_callback(result)
        
        # Le yield est plus pour le pipeline Scrapy standard, on s'appuie sur le callback
        # yield result 
    
    def errback(self, failure):
        """Gestion erreurs."""
        error_message = f"Erreur Scrapy/Playwright: {failure.value}"
        logger.error(f"‚ùå {error_message}")
        result = {
            "data": {target: None for target in TARGETS},
            "url": URL_BASE,
            "timestamp": datetime.now().isoformat(),
            "error": error_message
        }
        
        if self.result_callback:
            self.result_callback(result)
        
        # yield result


# ==============================
# GESTION REACTOR
# ==============================
def start_reactor():
    """D√©marrer le reactor Twisted."""
    logger.info("üîÑ D√©marrage reactor Twisted...")
    try:
        reactor_ready.set()  # Signaler que le reactor est pr√™t
        # L'installationSignalHandlers=False est crucial pour ne pas interf√©rer avec le processus parent (Gunicorn)
        reactor.run(installSignalHandlers=False) 
    except Exception as e:
        logger.error(f"‚ùå Erreur critique lors du d√©marrage du reactor: {e}")
    logger.info("üõë Reactor arr√™t√©")


def wait_for_reactor(timeout=10):
    """Attendre que le reactor soit pr√™t."""
    if not reactor_ready.wait(timeout=timeout):
        logger.error("‚ùå Timeout: reactor non pr√™t")
        raise TimeoutError("Reactor non initialis√© apr√®s le d√©lai imparti")
    logger.info("‚úÖ Reactor pr√™t")


# ==============================
# FONCTION DE SCRAPING
# ==============================
def scrape_and_save(sync_type='manual', scheduled_datetime=None):
    """Effectue le scraping et enregistre."""
    start_time = time.time()
    
    # 1. V√©rifier le lock (si un autre scraping est d√©j√† en cours)
    if not scraping_lock.acquire(blocking=False):
        logger.warning("‚ö†Ô∏è  Scraping d√©j√† en cours (lock actif)")
        return {
            "status": "warning",
            "message": "Scraping d√©j√† en cours",
            "sync_type": sync_type
        }
    
    try:
        # Capturer la date/heure de d√©but si non fournie (pour l'enregistrement DB)
        scraping_datetime = scheduled_datetime if scheduled_datetime is not None else datetime.now()
        
        logger.info("="*80)
        logger.info(f"üöÄ EXTRACTION ({sync_type}) - {scraping_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("="*80)
        
        # 2. Attendre que le reactor soit pr√™t (jusqu'√† 30s)
        try:
            wait_for_reactor(timeout=30)
        except TimeoutError as e:
            logger.error(f"‚ùå {e}")
            log_sync_operation(sync_type, 'failed', 0, str(e), time.time() - start_time)
            return {"status": "error", "message": str(e), "sync_type": sync_type}
        
        # 3. Pr√©parer le r√©sultat
        result_data = {"completed": False, "result": None, "lock": Lock()}
        
        def result_callback(result):
            """Callback quand le spider termine."""
            with result_data["lock"]:
                result_data["result"] = result
                result_data["completed"] = True
        
        # 4. Fonction de crawl Twisted
        @defer.inlineCallbacks
        def crawl():
            try:
                global runner
                if runner is None:
                    # Configuration du runner Scrapy
                    configure_logging({'LOG_ENABLED': False})
                    runner = CrawlerRunner(ShmetSpider.custom_settings)
                
                # Lancer le spider
                d = runner.crawl(ShmetSpider, result_callback=result_callback)
                # Attendre la fin du crawl
                yield d
                logger.info("‚úÖ Crawl termin√© (Twisted)")
            except Exception as e:
                logger.error(f"‚ùå Erreur crawl (Twisted): {e}")
                # Assurez-vous d'appeler le callback m√™me en cas d'erreur dans le crawl
                result_callback({
                    "data": {target: None for target in TARGETS},
                    "url": URL_BASE,
                    "timestamp": datetime.now().isoformat(),
                    "error": f"Erreur interne Twisted: {str(e)}"
                })

        # 5. Planifier le crawl dans le reactor
        # Ceci est CRUCIAL: le crawl doit √™tre appel√© depuis un thread du reactor
        reactor.callFromThread(crawl) 
        logger.info("üì§ Crawl planifi√© (dans le reactor thread pool)")
        
        # 6. Attendre le r√©sultat (blocage du thread actuel)
        timeout = 180  # 3 minutes
        elapsed = 0
        check_interval = 2
        
        while elapsed < timeout:
            time.sleep(check_interval)
            elapsed += check_interval
            
            with result_data["lock"]:
                if result_data["completed"]:
                    break
            
            if elapsed % 20 == 0:
                logger.info(f"‚è≥ Attente {elapsed}s...")
        
        with result_data["lock"]:
            if not result_data["completed"]:
                duration = time.time() - start_time
                logger.error(f"‚è±Ô∏è  TIMEOUT apr√®s {duration:.2f}s")
                log_sync_operation(sync_type, 'failed', 0, 'Timeout', duration)
                return {
                    "status": "error",
                    "message": f"Timeout apr√®s {duration:.2f}s",
                    "sync_type": sync_type
                }
            
            result = result_data["result"]
        
        # 7. Traiter et Sauvegarder
        if "error" in result:
            duration = time.time() - start_time
            error_msg = result.get("error")
            logger.error(f"‚ùå Erreur: {error_msg}")
            log_sync_operation(sync_type, 'failed', 0, error_msg, duration)
            return {"status": "error", "message": error_msg, "sync_type": sync_type}
        
        data = result.get("data", {})
        metals_updated = save_prices_to_db(data, result.get("url"), scraping_datetime)
        
        duration = time.time() - start_time
        
        status = 'success' if metals_updated == len(TARGETS) else ('partial' if metals_updated > 0 else 'failed')
        
        log_sync_operation(sync_type, status, metals_updated, None, duration)
        
        logger.info("="*80)
        logger.info(f"‚úÖ TERMIN√â ({duration:.2f}s) - {metals_updated}/{len(TARGETS)}")
        logger.info(f"    üìÖ Date enregistrement: {scraping_datetime}")
        logger.info("="*80)
        
        return {
            "status": status,
            "data": data,
            "metals_saved": metals_updated,
            "total_targets": len(TARGETS),
            "duration": duration,
            "sync_type": sync_type,
            "timestamp": result.get("timestamp")
        }
        
    except Exception as e:
        duration = time.time() - start_time
        logger.error(f"‚ùå Erreur g√©n√©rale: {e}")
        logger.error(traceback.format_exc())
        
        log_sync_operation(sync_type, 'failed', 0, str(e), duration)
        
        return {"status": "error", "message": str(e), "sync_type": sync_type}
        
    finally:
        scraping_lock.release()


# ==============================
# T√ÇCHE PLANIFI√âE
# ==============================
def scheduled_scraping_job():
    """T√¢che planifi√©e - lance la fonction de scraping dans un nouveau thread."""
    scheduled_time = datetime.now()
    
    logger.info("="*80)
    logger.info(f"‚è∞ T√ÇCHE PLANIFI√âE D√âCLENCH√âE - {scheduled_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("="*80)
    
    # Lancement dans un thread pour √©viter de bloquer le pool de threads de l'APScheduler
    def run_scraping_in_thread():
        try:
            result = scrape_and_save(sync_type='scheduled', scheduled_datetime=scheduled_time)
            logger.info(f"üìä R√©sultat scheduled: {result.get('status')} - M√©taux sauv√©s: {result.get('metals_saved')}")
        except Exception as e:
            logger.error(f"‚ùå Erreur critique dans thread planifi√©: {e}")
            
    thread = Thread(target=run_scraping_in_thread, daemon=True, name="ScheduledScrapingThread")
    thread.start()


# ==============================
# APPLICATION FLASK
# ==============================
app = Flask(__name__)

# D√©terminer le HOST pour Swagger (CRUCIAL pour le d√©ploiement)
# On utilise la variable d'environnement ou le lien fourni par l'utilisateur
DEPLOYED_HOST = os.environ.get('WEBSITE_HOSTNAME', 'api-exmetal.azurewebsites.net') 

# Configuration et initialisation de Swagger
swagger_config = {
    "headers": [],
    "specs": [{
        "endpoint": "apispec",
        "route": "/apispec.json",
        "rule_filter": lambda rule: True,
        "model_filter": lambda tag: True,
    }],
    "static_url_path": "/flasgger_static",
    "swagger_ui": True,
    "specs_route": "/docs"
}

swagger_template = {
    "swagger": "2.0",
    "info": {
        "title": "API Extraction Prix M√©taux",
        "description": "API pour extraire et g√©rer les prix des m√©taux depuis Shmet",
        "version": "3.1-CORRECTED",
        "contact": {"name": "Support API"},
    },
    # MISE √Ä JOUR CRITIQUE: Utiliser le host d√©ploy√© pour que Swagger UI fonctionne
    "host": DEPLOYED_HOST, 
    "basePath": "/",
    "schemes": ["http", "https"],
}

swagger = Swagger(app, config=swagger_config, template=swagger_template)

# ==============================
# ROUTES API
# ==============================
@app.route("/", methods=["GET"])
def home():
    """
    Accueil de l'API
    ---
    responses:
      200:
        description: Informations sur le service et les endpoints disponibles
        schema:
          type: object
          properties:
            service: {type: string}
            version: {type: string}
            endpoints: {type: object}
    """
    return jsonify({
        "service": "API extraction prix m√©taux",
        "version": "3.1-CORRECTED",
        "endpoints": {
            "/extract": "POST - Extraction manuelle",
            "/prices/latest": "GET - Derniers prix",
            "/prices/history": "GET - Historique",
            "/sync/logs": "GET - Logs",
            "/health": "GET - Sant√©",
            "/targets": "GET - Produits",
            "/docs": "GET - Documentation (Swagger UI)"
        }
    })

@app.route("/health", methods=["GET"])
def health_check():
    """
    V√©rification de la sant√© du service
    V√©rifie la connexion √† la base de donn√©es, l'√©tat du reactor Twisted et du scheduler APScheduler.
    ---
    tags:
      - Monitoring
    responses:
      200:
        description: Statut du service
        schema:
          type: object
          properties:
            status: {type: string, description: "healthy ou unhealthy"}
            database: {type: string, description: "connected ou disconnected"}
            reactor: {type: string, description: "running ou starting"}
            scheduler: {type: string, description: "running ou stopped"}
            timestamp: {type: string, format: date-time}
    """
    db_status = "unknown"
    try:
        conn = get_db_connection()
        conn.close()
        db_status = "connected"
    except:
        db_status = "disconnected"
    
    return jsonify({
        "status": "healthy",
        "database": db_status,
        "reactor": "running" if reactor_ready.is_set() else "starting",
        "scheduler": "running" if scheduler and scheduler.running else "stopped",
        "timestamp": datetime.now().isoformat()
    })

@app.route("/targets", methods=["GET"])
def get_targets():
    """
    Liste des produits et leur mapping interne
    ---
    tags:
      - Configuration
    responses:
      200:
        description: Liste des produits cibl√©s pour l'extraction
        schema:
          type: object
          properties:
            targets: {type: array, items: {type: string}}
            count: {type: integer}
            mapping: {type: object, description: "Mapping du nom du produit √† un type de m√©tal g√©n√©rique"}
    """
    return jsonify({
        "targets": TARGETS,
        "count": len(TARGETS),
        "mapping": METAL_MAPPING
    })

@app.route("/extract", methods=["POST"])
def extract_prices():
    """
    D√©clenchement manuel de l'extraction
    Lance la proc√©dure de scraping des prix depuis Shmet et les enregistre en base de donn√©es.
    ---
    tags:
      - Extraction
    responses:
      200:
        description: Extraction lanc√©e avec succ√®s ou d√©j√† en cours (warning)
        schema:
          type: object
          properties:
            status: {type: string, description: "success, partial, ou warning (si d√©j√† en cours)"}
            data: {type: object, description: "Prix extraits (cl√©s = noms des produits)"}
            metals_saved: {type: integer}
            total_targets: {type: integer}
            duration: {type: number}
            sync_type: {type: string}
            timestamp: {type: string, format: date-time}
      500:
        description: Erreur interne, timeout ou √©chec de la connexion/scraping
        schema:
          type: object
          properties:
            status: {type: string, description: "error"}
            message: {type: string}
            sync_type: {type: string}
    """
    logger.info("üéØ /extract (manuel)")
    # Lancement du scraping
    result = scrape_and_save(sync_type='manual')
    
    if result["status"] in ["success", "partial", "warning"]:
        return jsonify(result), 200
    else:
        # En cas d'erreur ou de timeout
        return jsonify(result), 500

@app.route("/prices/latest", methods=["GET"])
def get_latest_prices():
    """
    R√©cup√©ration des derniers prix
    R√©cup√®re le prix le plus r√©cent pour chaque type de m√©tal, ou l'historique r√©cent d'un type sp√©cifique.
    ---
    tags:
      - Prix
    parameters:
      - name: metal_type
        in: query
        type: string
        enum: [copper, zinc, tin]
        description: Filtre optionnel sur le type de m√©tal (si absent, retourne les derniers de tous les types)
    responses:
      200:
        description: Derniers prix enregistr√©s
        schema:
          type: object
          properties:
            status: {type: string}
            count: {type: integer}
            prices:
              type: array
              items:
                type: object
                properties:
                  metal_type: {type: string}
                  price: {type: number}
                  created_at: {type: string}
      500:
        description: Erreur de base de donn√©es
    """
    metal_type = request.args.get("metal_type")
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        if metal_type:
            # R√©cup√®re les 10 derni√®res entr√©es pour le type de m√©tal sp√©cifi√©
            query = "SELECT * FROM metal_prices WHERE metal_type = %s ORDER BY created_at DESC LIMIT 10"
            cursor.execute(query, (metal_type,))
        else:
            # R√©cup√®re la derni√®re entr√©e pour chaque type de m√©tal (en utilisant DISTINCT ON)
            query = "SELECT DISTINCT ON (metal_type) * FROM metal_prices ORDER BY metal_type, created_at DESC"
            cursor.execute(query)
        
        results = cursor.fetchall()
        cursor.close()
        conn.close()
        
        return jsonify({"status": "success", "count": len(results), "prices": results}), 200
        
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route("/prices/history", methods=["GET"])
def get_price_history():
    """
    R√©cup√©ration de l'historique des prix
    R√©cup√®re les prix sur une p√©riode ou avec une limite sp√©cifique.
    ---
    tags:
      - Prix
    parameters:
      - name: metal_type
        in: query
        type: string
        enum: [copper, zinc, tin]
        description: Type de m√©tal √† filtrer
      - name: days
        in: query
        type: integer
        default: 7
        description: Nombre de jours d'historique √† inclure
      - name: limit
        in: query
        type: integer
        default: 100
        description: Nombre maximum d'enregistrements √† retourner
    responses:
      200:
        description: Historique des prix filtr√©
        schema:
          type: object
          properties:
            status: {type: string}
            count: {type: integer}
            filters: {type: object}
            history:
              type: array
              items:
                type: object
    500:
      description: Erreur de base de donn√©es
    """
    metal_type = request.args.get("metal_type")
    days = request.args.get("days", default=7, type=int)
    limit = request.args.get("limit", default=100, type=int)
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        if metal_type:
            query = "SELECT * FROM metal_prices WHERE metal_type = %s AND created_at >= NOW() - INTERVAL '%s days' ORDER BY created_at DESC LIMIT %s"
            cursor.execute(query, (metal_type, days, limit))
        else:
            query = "SELECT * FROM metal_prices WHERE created_at >= NOW() - INTERVAL '%s days' ORDER BY created_at DESC LIMIT %s"
            cursor.execute(query, (days, limit))
        
        results = cursor.fetchall()
        cursor.close()
        conn.close()
        
        return jsonify({
            "status": "success",
            "count": len(results),
            "filters": {"metal_type": metal_type, "days": days, "limit": limit},
            "history": results
        }), 200
        
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route("/sync/logs", methods=["GET"])
def get_sync_logs():
    """
    R√©cup√©ration des logs de synchronisation
    Affiche l'historique des tentatives de scraping (manuel ou planifi√©).
    ---
    tags:
      - Monitoring
    parameters:
      - name: limit
        in: query
        type: integer
        default: 50
        description: Nombre maximum de logs √† retourner
    responses:
      200:
        description: Liste des logs de synchronisation
        schema:
          type: object
          properties:
            status: {type: string}
            count: {type: integer}
            logs:
              type: array
              items:
                type: object
    500:
      description: Erreur de base de donn√©es
    """
    limit = request.args.get("limit", default=50, type=int)
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        query = "SELECT * FROM sync_logs ORDER BY created_at DESC LIMIT %s"
        cursor.execute(query, (limit,))
        
        results = cursor.fetchall()
        cursor.close()
        conn.close()
        
        return jsonify({"status": "success", "count": len(results), "logs": results}), 200
        
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


# ==============================
# INITIALISATION
# ==============================
def initialize_app():
    """Initialiser le reactor et le scheduler dans le bon ordre."""
    global scheduler
    
    logger.info("üîß Initialisation...")
    
    # 1. D√©marrer le reactor dans un thread
    # Le run() du reactor est bloquant, il DOIT √™tre dans un thread s√©par√©
    reactor_thread = Thread(target=start_reactor, daemon=True, name="ReactorThread")
    reactor_thread.start()
    logger.info("‚úÖ Thread reactor lanc√©")
    
    # 2. Attendre que le reactor soit pr√™t
    try:
        wait_for_reactor(timeout=30)
    except TimeoutError:
        logger.error("‚ùå Le reactor n'a pas pu d√©marrer √† temps. Le scraping sera inutilisable.")
        return False # √âchec de l'initialisation
        
    # 3. D√©marrer le scheduler APR√àS le reactor
    if scheduler is None:
        scheduler = BackgroundScheduler()
        # D√©clenchement √† 9h10 (heure du serveur o√π il est d√©ploy√©)
        scheduler.add_job(
            func=scheduled_scraping_job,
            trigger=CronTrigger(hour=9, minute=10), 
            id='daily_scraping',
            name='Extraction quotidienne 9h10',
            replace_existing=True
        )
        scheduler.start()
        logger.info("‚è∞ Scheduler d√©marr√©: extraction planifi√©e √† 9h10 (heure du serveur)")
    
    logger.info("‚úÖ Initialisation termin√©e")
    return True


# ==============================
# POINT D'ENTR√âE (Critique pour le d√©ploiement)
# ==============================

# L'initialisation doit √™tre appel√©e lorsque le serveur WSGI (Gunicorn) d√©marre.
# Pour le d√©veloppement local (pour le test), on garde le bloc __main__.

if __name__ == "__main__":
    logger.info("="*80)
    logger.info("üöÄ D√âMARRAGE SERVEUR LOCAL DE D√âVELOPPEMENT (via app.run)")
    logger.info("="*80)
    
    if initialize_app():
        logger.info(f"üìä Documentation: http://{DEPLOYED_HOST}/docs (ou http://localhost:5000/docs en local)")
        logger.info(f"üéØ {len(TARGETS)} produits")
        logger.info("="*80)
        
        try:
            # use_reloader=False est important pour √©viter de d√©marrer le reactor deux fois en mode dev
            # threaded=True est requis car le scraping bloque le thread
            app.run(host="0.0.0.0", port=5000, debug=False, threaded=True, use_reloader=False)
        except (KeyboardInterrupt, SystemExit):
            if scheduler:
                scheduler.shutdown()
            logger.info("üõë Arr√™t du serveur")
        except Exception as e:
            logger.error(f"‚ùå Erreur fatale du serveur: {e}")
            raise
    else:
        logger.error("‚ùå Arr√™t car l'initialisation du Reactor a √©chou√©.")
